This note provides a description of the problem and related considerations; in association to a pursuit to figure out how to define a solution.  The concept currently labelled 'Streaming Ai Packages' is a foundational consideration about how to develop a foundational software environment that is designed to support Human Centric AI systems, with the sociosphere protections that are noted only briefly below.  More information about the sociosphere AI / Safety protocols are described elsewhere. 

### Laymens Explaination.

In the matrix, NEO sits on a chair and gets a package of information installed into his head.

<iframe width="560" height="315" src="https://www.youtube.com/embed/w_8NsPQBdV0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

In the models that are being produced to support 'Human Centric AI' the information isn't uploaded into a persons brain; but rather, into their 'Ai Agent', such as is described as Webizen or otherwise by others, overtime - if not apparent at the present time.  

Whilst big global systems like the Chat interface provided by OpenAI are operating on big systems that have available to them lots of resources / servers;  Human Centric AI, in providing personal and private systems foundationally; as to thereafter support the use of AI systems with others, requires a means to get and process the information privately; prior to, thereafter being able to be instructed to initiate a sharing event, rather than a process where the thoughts must be shared prior to being able to 'think' about or 'do' anything at all.  

The global AI systems; are 'trained' (developed) in a way that seeks to cater for a vast array of different applications that it may be used for by whomever in the world makes use of it; and in-turn, provides to it all their thoughts and creative works, that is in-turn added to the 'hive mind' like model.   This is part of the reason why the models are so big; whilst the implication is also that they're very difficult for people to use independently due to the cost of deployment; particularly if they want it running on their own systems (privately).

Now, in-order to produce the foundational requirements for the Webizen AI project and any other related project that may seek to employ the tooling for good / useful purposes, etc. there needs to be a way to process and support basic AI functions that are fundamentally based upon a means to process and respond to users in a manner that relates to the use of language; but moreover, there is a meaningful relationship between language and the development of our human civilisations.  Therein, the process of producing solutions that address language in particular; as is then better able to be used to construct models that use it as a dependency,  becomes a fairly large and complex system.

### Basic Concept

The type of challenge that is presented when considering these problems is not unlike the sorts of issues that had to be contended with many years ago; in relation to streaming video and in-turn also, software / games.  Therein, the solution that was defined - in lay-speak, didn't require a person to download the entire video library in-order to watch one film; indeed it didn't even require a user to download the entire film, prior to being able to start watching it.  Whilst the nature of AI systems are not linear in the same way, I am now trying to work through the concept of how these sorts of ideas might apply to advaneced AI 'models' and indeed also, whether it is about 'one model' or a plurity of models or dynamic systems; that may have elements that are trained or compiled, whilst built with dependency models that are not.  Presently - i don't know, that's why i'm writing this note about it. 

I'll start with a summary overview for people who aren't good with these sorts of things; then, continue with my thinking atm, on the topic, after the introductory summary part of the note.


## Background

Artificial Intelligence systems are becoming increasingly visable in the public domain whilst the nature of their influence upon society remains largely unknown.  Whilst there are many exceptional scientists who have been working with Artificial Intelligence related technologies over many years, the ability to support private and personal AI services remains out of reach. 

What is AI? 

There are many different types of technologies that are broadly provided the label 'AI" or 'Artificial Intelligence'.  There is a continual propensity for users and creators alike, to seek to place upon these creative works; names relating to characteristics and/or functional behaviours that ordinarily associate with the namonclature related to descriptions about the being of humans and other natural life.  Many of the terms used, whilst sought to be considered on an endearing level; relate to various behavioural characteristics associated with consciousness and/or terminology associated with what is known to be part of 'being' and/or mind, in association to life; both as human beings, but also as is exhibited more broadly.  

The vast majority of 'AI' systems and/or technologies, do not make use of Quantum Computing hardware in any meaningful way; simply, because it is not yet commonly available and/or sufficiently evolved, at least not in any known commercial and/or non-military area.

Whilst complex, the systems at a root-hardware layer rely upon semiconductors / transisters, which in-turn provides a programming layer via binary gates.

<iframe width="560" height="315" src="https://www.youtube.com/embed/gUmDVe6C-BU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Artificial Intelligence systems are generally implemented using software rather than hardware; although there are some exceptions, and here is resource that provides links to some of those examples,
https://www.mdpi.com/journal/electronics/sections/AICAS

An example of hardware implemented technology designed to assist AI software includes CUDA which is commonly associated with Graphics Cards, as well as TensorCores.  Emergent   Neuromorphic Circuits are becoming available, however they're not commonly available yet.

Functionally; most AI is software, and whilst it depends upon hardware, as does any form of software, the majority of the functionality is written as software code.  

There are many different catagories of "AI", which is why the term is considered by many to be a 'dirty term'.  Some of the top-level categories are;

-   Neural Network 
-   Machine learning 
-   Deep learning 

And, there are many, many different sub-categories that exist within these fields alongside others that may be defined as being part of other different categories. 

The main difference between software, that would otherwise be called software or software programs; and software, that is called 'AI' and has some sort of related functionality attributed to it, is that 'AI' algorithms are intended to produce a 'learning' *like* functional capacity in some way that provides usefulness - although, the latter concept (usefulness) may be subjectively determined.   None of these processes are able to occur without providing the software information, sensors and fundamentally therein - data.  The difference between data and information is subjective for human beings in similar yet different ways that have been applied to machines via the works broadly known as AI..

### Information and DataTypes

AI systems have and are being designed to consume an array of different types of information or data; ranging from images and image streams (video) audio, code and written languages.  One of the critical areas of importance is to distinguish un-structured data or information; from structured data, or information.  The latter employs various techniques to provide a labelling framework that is in-turn able to be used to provide classifications of the information provided to algorithms; in a way, that assists in algorithm development and use.

### The World of Algorithms

The vast majority of what is defined as 'ai' is algorthms.  The combination of providing software algorithms information (structured, semi-structured, etc.) and enormous amounts of power (computational capabilities) is what ends-up defining the output that we call 'AI'. 

These algorithms employ classical physics and mathematics to process vast amounts of information that has been produced and is available to the software to consume. 

### Cloud AIs

"Cloud Computing" is effectively a nice way of defining a computing environment that connects users to computing resources that are most-often owned by some other party.  

These systems may be functionally supported by thousands to hundreds of thousands or more; computers, that are located in various places geographically.  Cloud AIs are software programs that exist on these massive systems; and are functionally 'owned' by the platform provider whilst engaged in communications activities via defined rules. 

### HumanCentric AIs

A Human Centric AI is a form of AI that has been developed in a particular way to provide a range of different sorts of characteristics.  

It is able to;
1. Be Beneficially owned by the human being it is intended to be used by. 
2. It is able to maintain secrecy of a persons own mindful activities; in so far as any such activities do not relate to any other person or entity in a way that may effect them.
3. Maintain Permissions that have been applied between users when communications events relate to shared activities, events, works, etc.
4. It is able to support SocioSphereAI requirements.

### SocioSphereAI

SocioSphere AI functions and processes, otherwise considered to be 'Safety Protocols', are mechanisms that are designed to consider and support various requirements that otherwise exist within our natural world. These systems describe the inter-personal relationships that exist between living entities and in-particular therein - between members of our human family, as does relate to societal systems and related expectations, precautions, requirements and other safety measures. 

### Software Languages

There are several different software languages that are usefully employed when defining AI systems.  There is not any one software language that is considered 'best equipped' to do it all.   AI systems are expected to develop to produce improvements overtime. 

#### Business Systems

It is essential to consider the implications of various business systems that are applied to the use of technology; which has various impactful repercussive effects upon the 'fit for purpose' usefulness of how those technologies have been produced and in-turn applied.  

### Introductory Summary

AI is today, mostly operating as 'CloudAI' systems.  The most well-known variants are operating globally.  What does not exist today, is HumanCentricAI & related SocioSphereAI requirements to support HumanCentricAI.   Whilst there are alot of building-blocks that require implementation in a particular and therefore specified manner; one of the issues that is considered difficult to address, is that the complexity of the models required to support high-quality AI inferencing solutions, are presently very large.  The consequence of this is that they're only able to operate on systems that have much more resources available than is reasonably considered available for 'home users' or in other words, natural persons in their private capacity; or in other words, means to support Human Centric AI. 

The implication of Human Centric AI; does not replace other forms of AI systems; rather, the intended purpose is to provide grounding for the development of societal systems that will in-turn grow to become dependant upon the use of AI in a way that is akin to electricity. 

As such; the complex problem that this note is seeking to illustrate, is about the methods through which it MAY be possible, to develop a solution that supports the ability for users to stream AI 'models' or 'packages' to systems that they control; and are privately employable for the user.  In-turn also, how these systems may in-turn also support, the ability to share and operate on a permissive basis, throughout and between communities; based upon attributes and other discriminating factors.  Noting, this is intended to be about empowerment, rather than harms or discrimination, etc; but therein, the rules about why and how these sorts of functions are achieved, become instrumental to the development of systems that are equipped to achieve any sort of materially protective & positive set of goals.

## Design Challenges & Considerations

As noted at the start of this document; there's a challenge about how to form useful software that doesn't require an individual to be 'wired into' the broader global 'cloud' 'hive-mind'. 

### Notes about Requirements

There are several 'langauge models' that exist; and have been produced in various formats.  There are differences between the models that exist, and the ability to produce a 'model' (dislike that term, but anyhow) that better takes into consideration various complexities, that are linked with spatio-temporal, sociology factors that often thereafter invokes factors that relate to other languages; whether written or pictorial, results in a complex series of inputs.

Furthermore; as it is said, 'history is told by the winner', which is a material factor that has various implications; that may profoundly determine, that the popular belief about the provonance or meaning of a concept, may not be the accurate objective description of it, but rather a common-place subjective opinion, or objective association of the use of a word or phrase in a particular social spatio-temporal context.  

An Personal AI agent; should ideally be equipped to support the means to inform the human user of circumstances where the use of particular language in a particular way, may have unintended consequences due to differences between persons and/or groups, in association to the use of any such language.  

Then the extension that is built upon defining an appropriate language AI process (sense) for Human Centric AI (ie: Webizen); thereafter, has an impact on how it is in-turn used by anything that is built on-top of it; and/or, in a manner that makes use of it as a dependency.

In seeking to figure out how to form a model that supports both the independent and personal and thereafter - use with communities, i've started writing this note... 

It is going to be important to also consider how to support maintainence of the tooling, alongside hygiene and various other factors. 

### Compression

The use of techniques to reduce the size of the files and programs is central to the methods considerations.  These considerations are broader than simply transport related compression methodologies.  The coding schema should consider how techniques that have the effect of reducing the code-base might be effectively applied.  If the code-base is able to assume a degree of existing or predicate information exists within the database, this may be in-turn employed in a manner that reduces both the filesizes and the amount of memory that's used; but, the implications with respect to the processing costs, are something to be considered.

There's a question about whether and how unique identifiers might be created which have the result of significantly reducing the size of queries otherwise associated with the texts.

Question becomes; whether and how look-up tables might be used; and/or how they may already be in use via underlying software.

Compression doesn't matter so much if the model is relatively small and used rarely; however, when the opposite is the case, there's an array of important benefits.

### Decentralised Cryptography Considerations

There's an array of factors that would benefit from ensuring the methods take into account the idea that there are decentralised and federated schemas in use; in addition to version control related considerations.  The version control issues, end-up being part of the temporal solutions, as changes must occur overtime; rather than simultaniously. 

Yet, when there is an assumption that a record is the same as is intended; then, there should be some way of verifying / validating that the assumption is correct.

Another issue that is quite important; is to ensure that there are strong cryptographically enhances measures to control the use of the Networked AI components by human agents.

This will in-turn require an array of 'safety protocols' that seek to expressly state what the intended characteristics are and what conditions may present an alarm and/or stop process.

### Theory linked considerations.

Workflow concepts.

User has basic app installed. 
App has a database capability. 
App has a capacity to run models somehow.
models are defined as components. 

Models evolve overtime using 'spare' computational capacity.

App has command set for pulling resources from the web in parts from nodes across the network.  

Linked Considerations: 

SETI - the Seti App distributed processing across various nodes.  




