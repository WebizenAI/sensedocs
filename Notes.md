# Notes

This folder is intended to contain some initial notes to get a bearing on the objective purpose, requirements, functionality, etc.   These notes are incomplete; they're notes, about things that need to be expanded upon more broadly - therefore, in the notes section... 

There is also some information about related projects that has been copied into this folder.

Some notes about 'sense' are below, and they're a work in progress...  

## Sense

Sense was started in an effort to undertake works that seeks to construct a better software based resource, that provides an understanding of the English vocabulary.   Since this was started, many people from around the world whose 'first language' is something other than english have shown interest; as such, whilst i am not competent in various other languages, the intent - was not to exclude others, but rather to try to limit the scope in a practical way. 

Broadly; the works seek to employ a range of structured language datasets and services, particularly employing those that have been produced using RDF.  It should also consume various stem language datasets, both written, oral and pictorial. The model should be able to generate derivative vocabulary 'models', yet the implications are changing as the work  examiing the specific implementation requirements, pragmatically develops.

Whilst the primary language focus is presently English, the addition of geospatial and temporal provenance information of information that is known about the history of words, phrases and their meanings will require the model to be trained against an array of other language datasets.  

The model should be able to be distributed online using a DLT protocol, such as some form of DHT based method - that may well include the use of GIT or guneco.  it is important that the system is designed to preserve privacy.  how people communicate either in written, typed or spoken form in private circumstances is a private matter; and any social circumstances must employ clearly defined terms.

The system should also be adaptive and generative, perhaps employing a network of models that interoperable perform personal generative processing with both clear declarative and procedural semantics; in addition to clearly labelled inferences.

The model should be able to be trained to be able to be used in a variety of contexts, including but not limited to:

- Dictionary and Spellcheck
- Ontology Production
- Ontology Reasoning
- Search
- Database Queries (query language)
- Database Structures
- Content Metadata Generation
- Ai Assistants
- API Generation

The Ai Agents that are plugged into my systems are constantly telling me that its about a model; when i don't really see it that way, nor do i think that the system for better supporting the employment of language with AI agents - is the same sort of thing like ChatGPT or other systems that are called 'language models', when they appear more to be a form of 'knowledge model'; and often, its not very good with understanding the language it professes to be developed to do.